---

layout: post
title: 从一元导数的几何意义理解梯度下降
tags: mathematics
author: Liucheng Xu
disqus: "y"
math: y
donate: y
published: true

---

* TOC
{:toc}

最优化问题及机器学习算法中非常重要的一部分，很多机器学习算法的核心都是在处理最优化问题。

梯度下降法（gradient descent）是一种常用的一阶（first-order）优化方法，是求解无约束问题最简单、最经典的方法之一。

机器学习中常会用梯度下降法求解一个目标函数的优化问题，我们所追求的是目标函数能够快速收敛或到达一个极小值点。而随机梯度法操作起来也很简单，不过是求偏导数而已，但是为什么是这样呢？为什么算个偏导数就能说下降得最快？初期并不很明了，后来看过一些数学相关的知识才稍微明白了一点，以下内容算是一个理解梯度的渐进过程。如果不当之处，欢迎指正。

以下关于梯度下降法，导数，偏导数的内容可在维基百科中找到，关于方向导数与梯度的内容可在高等数学书中找到。


## 预备知识

既然是从导数的几何意义谈起，那么肯定要先了解导数的几何意义是什么。 下面文章的安排首先是简要介绍了梯度下降法，接着是导数 --> 偏导数 --> 方向导数 --> 梯度， 然后理解方向导数与梯度之间的关系， 每一步如果都明明白白的话，相信对于梯度的概念就断然不是只知道计算公式那么死板了。 

对于导数与偏导数大多数人应该都很熟悉， 请重点关注**方向导数与梯度**， 这是关键所在。

### 梯度下降法介绍

梯度下降法（Gradient descent）是一个用于寻找最小化成本函数（或目标函数）的参数值的最优化算法。当无法通过分析计算得到函数的最优解时，我们可以使用梯度下降法求解该问题。

#### wiki

梯度下降法，基于这样的观察：如果实值函数$F(\mathbf{x})$在点$\mathbf{a}$处可微且有定义，那么函数$F(\mathbf{x})$在$\mathbf{a}$点沿着梯度相反的方向 $-\nabla F(\mathbf{a})$ 下降最快。

因而，如果$\mathbf{b}=\mathbf{a}-\gamma\nabla F(\mathbf{a})$
对于$\gamma>0$为一个够小数值时成立，那么$F(\mathbf{a})\geq F(\mathbf{b})$。

考虑到这一点，我们可以从函数F的局部极小值的初始估计 $$ \mathbf{x}_0 $$ 出发，并考虑如下序列 
$$\mathbf{x}_0， \mathbf{x}_1， \mathbf{x}_2， \dots$$ 

使得 $$\mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F\left(\mathbf{x}_n\right)  \ n \ge 0$$。

因此可得到
$$F(\mathbf{x}_0)\ge F(\mathbf{x}_1)\ge F(\mathbf{x}_2)\ge \cdots$$

如果顺利的话序列收敛到期望的极值。注意每次迭代步长$\gamma$可以改变。

下面的图片示例了这一过程，这里假设F定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线（水平集），即函数F为常数的集合构成的曲线。红色的箭头指向该点梯度的反方向。（一点处的梯度方向与通过该点的等高线垂直）。沿着梯度下降方向，将最终到达碗底，即函数F值最小的点。

![partial](/assets/img/blog/2016/04-15/gd4.png)


梯度下降法也有一些缺点：

- 靠近极小值时速度减慢。
- 直线搜索可能会产生一些问题。
- 可能会'之字型'地下降。

#### 批量梯度下降法

所有的监督机器学习算法的目标都是利用已知的自变量$X$数据来预测因变量$Y$的值。所有的分类 ( classification ) 和回归 ( regression ) 模型都是在处理这个问题。

机器学习算法会利用某个统计量来刻画目标函数的拟合情况。虽然不同的算法拥有不同的目标函数表示方法和不同的系数值，但是它们拥有一个共同的目标——即**通过最优化目标函数来获取最佳参数值**。

线性回归(linear regression)和逻辑回归(logistic regression)模型是利用梯度下降法来寻找最佳参数值的经典案例。

我们可以利用多种衡量方法来评估机器学习模型对目标函数的拟合情况。成本函数法是通过计算每个训练集的预测值和真实值之间的差异程度(比如残差平方和)来度量模型的拟合情况。

我们可以计算成本函数中每个参数所对应的导数值，然后通过上述的更新方程进行迭代计算。

批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，具体思路为**更新每一个参数时使用所有的样本进行更新**。在梯度下降法的每一步迭代计算后，我们都需要计算成本函数及其导数的情况。每一次的迭代计算过程就被称为一批次（batch）。

批量梯度下降得到的是一个全局最优解，但是每迭代一次，都要用到训练集所有的数据，如果样本数目m很大，可想而知其迭代速度。这便也引出了下面的随机梯度下降法

#### 随机梯度下降法

处理大规模的数据时，梯度下降法的运算效率非常低。
因为梯度下降法在每次迭代过程中都需要计算训练集的预测情况，所以当数据量非常大时需要耗费较长的时间。

当你处理大规模的数据时，你可以利用随机梯度下降法来提高计算效率。
该算法与上述梯度下降法的不同之处在于它**对每个随机训练样本都执行系数更新过程**，而不是在每批样本运算完后才执行系数更新过程。

随机梯度下降法 ( Stochastic Gradient Descent， 简称SGD) 的第一个步骤要求训练集的样本是随机排序的，这是为了打乱系数的更新过程。因为我们将在每次训练实例结束后更新系数值，所以系数值和成本函数值将会出现随机跳跃的情况。通过打乱系数更新过程的顺序，我们可以利用这个随机游走的性质来避免模型不收敛的问题。

除了成本函数的计算方式不一致外，随机梯度下降法的系数更新过程和上述的梯度下降法一模一样。

对于大规模数据来说，随机梯度下降法的收敛速度明显高于其他算法，通常情况下你只需要一个小的迭代次数就能得到一个相对较优的拟合参数。

### 导数

导数（Derivative）是微积分学中重要的基础概念。一个函数在某一点的导数描述了这个函数在这一点附近的**变化率**。导数的本质是通过**极限**的概念对函数进行局部的线性逼近。

**物理意义上表示函数值在这一点的变化率。**

### 偏导数

在数学中，一个**多变量**的函数的偏导数是它关于其中一个变量的导数，而**保持其他变量恒定**（相对于全导数，在其中所有变量都允许变化）。

假设$ƒ$是一个多元函数。例如：

 $$z = f(x， y) = x^2 + xy + y^2$$

$f = x^2 + xy + y^2$的图像。我们希望求出函数在点（1， 1， 3）的对x的偏导数；对应的切线与xOz平面平行。
因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。**偏导数就是选择其中一条切线**，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。

![partial](/assets/img/blog/2016/04-15/partial.png)

一种求出这些切线的好办法是把其他变量视为常数。例如，欲求出以上的函数在点（1， 1， 3）的与xOz平面平行的切线。上图中显示了函数$f = x^2 + xy + y^2$的图像以及这个平面。下图中显示了函数在平面y = 1上是什么样的。我们把变量y视为常数，通过对方程求导，我们发现$ƒ$在点（x， y， z）的。我们把它记为：
$\frac{\partial z}{\partial x} = 2x+y$，于是在点（1， 1， 3）的与xOz平面平行的切线的斜率是3。$\frac{\partial f}{\partial x} = 3$ 在点（1， 1， 3），或称“f在（1， 1， 3）的关于x的偏导数是3”。

![partial](/assets/img/blog/2016/04-15/y=1.png)

**在物理意义上偏导数即为函数在坐标轴方向上的变化率。**

### 方向导数

方向导数是分析学特别是多元微积分中的概念。一个标量场在某点沿着某个向量方向上的方向导数，描绘了该点附近标量场沿着该向量方向变动时的瞬时变化率。方向导数是偏导数的概念的推广。

方向导数定义式：

![partial](/assets/img/blog/2016/04-15/zh.png)

方向导数计算公式（在推导方向导数与梯度关系时用到）：

![partial](/assets/img/blog/2016/04-15/gd5.png)

**物理意义上方向导数为函数在某点沿着其他特定方向上的变化率**。

### 梯度

在一个数量场中，函数在给定点处沿不同的方向，其方向导数一般是不相同的。那么沿着哪一个方向其方向导数最大，其最大值为多少，这是我们所关心的，为此引进一个很重要的概念 --> 梯度。**假设函数在其一点$p_0$处，那么它沿哪一方向函数值增加的速度能够最快？**

![partial](/assets/img/blog/2016/04-15/gd.png)

看到这里，再来看一下题目 “从导数的物理意义理解梯度”，更准确点，应当是**从方向导数的物理意义理解梯度**。方向导数指的的沿着某一特定方向的变化率，而梯度是指无数个方向上一个特别的方向导数。如果直接告诉你结论：梯度的特别之处在于是指方向导数最大的那个方向，那么便很容易理解开头的一些问题，既然梯度代表方向导数最大，自然沿着梯度方向变化率最快，函数值变化越明显。



## 为什么沿着梯度方向，函数值增加最快

这里对为什么梯度是方向导数最大的方向进行证明：

![partial](/assets/img/blog/2016/04-15/gd2.png)

![partial](/assets/img/blog/2016/04-15/gd3.png)

## 总结

函数在某一点处的方向导数在其**梯度方向上达到最大值**，此最大值即梯度的范数。

这就是说，**沿梯度方向，函数值增加最快**。同样可知，方向导数的最小值在梯度的相反方向取得，此最小值为最大值的相反数，从而**沿梯度相反方向函数值的减少最快**。详细内容可见：[方向导数与梯度](http://math.fudan.edu.cn/gdsx/KEJIAN/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.pdf)。

在机器学习中往往是最小化一个目标函数 $L(\Theta)$，理解了上面的内容，便很容易理解在梯度下降法中常见的参数更新公式：

$$\Theta = \Theta - \gamma \frac{\partial L}{\partial \Theta}$$

$\gamma$ 在机器学习中常被称为学习率 ( learning rate )， 也就是上面梯度下降法中的步长。

通过算出目标函数的梯度（算出对于所有参数的偏导数）并在其反方向更新完参数 $\Theta$ ，在此过程完成后也便是达到了函数值减少最快的效果，那么在经过迭代以后目标函数即可很快地到达一个极小值。如果该函数是凸函数，该极小值也便是全局最小值，此时梯度下降法可保证收敛到全局最优解。
